# **CURE: Controlled Unlearning for Robust Embeddings**

CURE is a **lightweight framework** that reduces **conceptual shortcuts** in pre-trained language models by disentangling concept-related signals from content information, then controlling their influence during inference. Experiments on **IMDB** and **Yelp** show notable gains in robustness with minimal overhead.  

This repository currently contains Jupyter notebooks to reproduce the core idea of controlled unlearning. The repo includes:

- `concept_classification.ipynb`
- `cure.ipynb`
- `small_network.ipynb`

**Paper:**  
**CURE: Controlled Unlearning for Robust Embeddings â€” Mitigating Conceptual Shortcuts in Pre-Trained Language Models**  
*Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*  
[arXiv:2509.05230](https://arxiv.org/abs/2509.05230)
