{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b4c85f-0c94-4605-8990-7c1c751c0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2, 3\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import nbimporter\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from small_network import SmallNetwork, InverseNetwork, kl_uniform_loss, combined_loss, init_weights\n",
    "from concept_classification import ConceptClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a9ec8d-aa90-474d-bb13-6a9c33a4e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "device0 = torch.device(\"cuda:0\")\n",
    "device1 = torch.device(\"cuda:1\")\n",
    "device2 = torch.device(\"cuda:2\")\n",
    "device3 = torch.device(\"cuda:3\")\n",
    "\n",
    "device=device1\n",
    "\n",
    "model_name=\"distilbert/distilbert-base-uncased\"   # microsoft/mpnet-base, distilbert/distilbert-base-uncased\n",
    "REVERSE=True # True: debiasing  False:Enhancing\n",
    "resconstruction=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad898222-e565-42e7-933e-14f8a08f7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SwiGLU Activation Function\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gelu(self.linear1(x)) * self.linear2(x)\n",
    "\n",
    "def contrastive_loss(cos_sim, margin=0.1, reverse=False):     # False: debiasing, True: enhance the shortcuts\n",
    "    if reverse:\n",
    "        return torch.mean(torch.relu(1 - cos_sim - margin))\n",
    "    loss = torch.mean(torch.relu(cos_sim - margin))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e07a89-8c0b-461a-8dc6-7744c995ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaSentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=2, model_name=\"roberta-base\", small_network=None, debiasing_module=True):\n",
    "        super(RobertaSentimentClassifier, self).__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Small Network\n",
    "        self.small_network = small_network\n",
    "\n",
    "        self.projection = nn.Linear(hidden_size, hidden_size)  # Adjust input size for concatenated embeddings\n",
    "        self.swiglu = SwiGLU(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size , num_labels)\n",
    "        self.debiasing_module=debiasing_module\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Forward pass through RoBERTa\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Forward pass through Small Network\n",
    "        if self.small_network:\n",
    "            reembedded_states = self.small_network(hidden_states)  # Shape: [batch_size, hidden_size]\n",
    "        else:\n",
    "            reembedded_states = torch.zeros_like(hidden_states)\n",
    "\n",
    "        concept_proj=self.projection(hidden_states)\n",
    "        content_proj=self.projection(reembedded_states)\n",
    "        cos_sim = F.cosine_similarity(concept_proj, content_proj, dim=-1)\n",
    "        \n",
    "        # Concatenate original RoBERTa embeddings and reembedded outputs\n",
    "        #combined_states = torch.cat([hidden_states, reembedded_states], dim=-1)  # Shape: [batch_size, hidden_size * 2]\n",
    "        if self.debiasing_module:\n",
    "            x = self.swiglu(concept_proj)\n",
    "        else:\n",
    "            x = self.swiglu(content_proj)\n",
    "        logits = self.fc(x)\n",
    "        return logits, cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b179bb-5c99-426e-968d-ab7b105f4881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "roberta_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9748c8b0-d958-4d74-9b4a-431f7cafd332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f63acd9b014395b166c5495a0e0efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbc3d5ec68f43bf9d5eedd809cee437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c115e0b98c943fa92135abb6c57ae10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1280\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 320\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset for Sentiment Classification\n",
    "\n",
    "# Tokenization function\n",
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['text'], padding=\"max_length\", truncation=True, max_length=32) # 记得改这里\n",
    "\n",
    "# Load dataset from CSV\n",
    "def load_data_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df[\"clean_text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Load train and test datasets\n",
    "if REVERSE:\n",
    "    train_texts, train_labels = load_data_from_csv(\"group_a_train.csv\")\n",
    "    test_texts, test_labels = load_data_from_csv(\"group_b_test.csv\")\n",
    "else:\n",
    "    train_texts, train_labels = load_data_from_csv(\"group_a_train.csv\")\n",
    "    test_texts, test_labels = load_data_from_csv(\"group_a_test.csv\")\n",
    "    \n",
    "# Split the training data into training and validation sets, 80% train, 20% validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "sentiment_train_data = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "sentiment_val_data = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
    "sentiment_test_data = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
    "\n",
    "# Tokenize the datasets\n",
    "sentiment_train_data = sentiment_train_data.map(tokenization, batched=True)\n",
    "sentiment_val_data = sentiment_val_data.map(tokenization, batched=True)\n",
    "sentiment_test_data = sentiment_test_data.map(tokenization, batched=True)\n",
    "\n",
    "# Set the format for PyTorch DataLoader compatibility\n",
    "columns = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "sentiment_train_data.set_format(type=\"torch\", columns=columns)\n",
    "sentiment_val_data.set_format(type=\"torch\", columns=columns)\n",
    "sentiment_test_data.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_sentiment = DataLoader(sentiment_train_data, batch_size=16, shuffle=True)\n",
    "val_loader_sentiment = DataLoader(sentiment_val_data, batch_size=16, shuffle=False)\n",
    "test_loader_sentiment = DataLoader(sentiment_test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "print(sentiment_train_data)\n",
    "print(sentiment_val_data)\n",
    "print(sentiment_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91bed80e-fd22-4b03-ba5b-3b6682a71929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for small network\n",
    "data_path_small_network = \"imbalanced_concepts.csv\"  # Replace with your CSV path\n",
    "data_small_network = pd.read_csv(data_path_small_network)\n",
    "\n",
    "# Tokenize and process data\n",
    "def tokenize_data(reviews):\n",
    "    tokenized_data = tokenizer(reviews, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_data[\"input_ids\"]\n",
    "    attention_mask = tokenized_data[\"attention_mask\"]\n",
    "    return TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "dataset_small_network = tokenize_data(data_small_network['clean_text'].tolist())\n",
    "data_size_small_network = len(dataset_small_network)\n",
    "train_size_small_network = int(0.8 * data_size_small_network)\n",
    "val_size_small_network = data_size_small_network - train_size_small_network\n",
    "\n",
    "train_data_small_network, val_data_small_network = random_split(dataset_small_network, [train_size_small_network, val_size_small_network])\n",
    "\n",
    "train_loader_small_network = DataLoader(train_data_small_network, batch_size=16, shuffle=True)\n",
    "val_loader_small_network = DataLoader(val_data_small_network, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "035dc364-a195-4f28-9f33-85789bae9c11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1478080/3532265686.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  concept_classifier.load_state_dict(torch.load(\"concept_classifier.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Inverse Network: 100%|████████████████████████████████████████████████| 289/289 [00:06<00:00, 47.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inverse Network Train Loss: 0.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Small Network: 100%|██████████████████████████████████████████████████| 289/289 [00:06<00:00, 48.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Small Network Train Loss: 0.3721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Small Network: 100%|██████████████████████████████████████████████████| 289/289 [00:05<00:00, 56.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Small Network Train Loss: 0.3767\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Inverse Network: 100%|████████████████████████████████████████████████| 289/289 [00:06<00:00, 45.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inverse Network Train Loss: 0.2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Small Network: 100%|██████████████████████████████████████████████████| 289/289 [00:06<00:00, 46.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Small Network Train Loss: 0.2191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Small Network: 100%|██████████████████████████████████████████████████| 289/289 [00:05<00:00, 48.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Small Network Train Loss: 0.2195\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Inverse Network: 100%|████████████████████████████████████████████████| 289/289 [00:05<00:00, 55.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inverse Network Train Loss: 0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Small Network: 100%|██████████████████████████████████████████████████| 289/289 [00:05<00:00, 48.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Small Network Train Loss: 0.1292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Small Network: 100%|██████████████████████████████████████████████████| 289/289 [00:04<00:00, 58.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Small Network Train Loss: 0.1294\n"
     ]
    }
   ],
   "source": [
    "lambda_reconstruction=0.9\n",
    "\n",
    "start_time=time.time()\n",
    "# Initialize models\n",
    "small_network = SmallNetwork(input_dim=768, bottleneck_dim=384).to(device)\n",
    "inverse_network = InverseNetwork(input_dim=768, bottleneck_dim=384).to(device)\n",
    "\n",
    "# Apply initialization\n",
    "#small_network.apply(init_weights)\n",
    "#inverse_network.apply(init_weights)\n",
    "\n",
    "concept_classifier = ConceptClassifier(num_labels=2, model_name=model_name).to(device)\n",
    "concept_classifier.load_state_dict(torch.load(\"concept_classifier.pt\"))\n",
    "concept_classifier.eval()  # Freeze concept classifier during small network training\n",
    "\n",
    "# Optimizers\n",
    "small_network_optimizer = optim.AdamW(small_network.parameters(), lr=0.0001)\n",
    "inverse_network_optimizer = optim.AdamW(inverse_network.parameters(), lr=0.0003)\n",
    "\n",
    "\n",
    "# Early stopping and metrics\n",
    "patience = 3\n",
    "best_val_loss = float(\"inf\")\n",
    "wait = 0\n",
    "\n",
    "train_losses_small = []\n",
    "train_losses_sentiment = []\n",
    "val_losses = []\n",
    "concept_classifier.eval()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 3\n",
    "num_inverse_epochs=1\n",
    "num_small_epochs=2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    if resconstruction:\n",
    "        # Step 0: Train inverse model to reconstruct the inputs\n",
    "        small_network.eval()\n",
    "        inverse_network.train()\n",
    "        for _ in range(num_inverse_epochs):\n",
    "            for batch in tqdm(train_loader_small_network, desc=\"Training Inverse Network\"):\n",
    "                input_ids, attention_mask = [x.to(device) for x in batch]\n",
    "        \n",
    "                # Forward pass through RoBERTa\n",
    "                with torch.no_grad():\n",
    "                    roberta_output = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    roberta_embeddings = roberta_output.last_hidden_state\n",
    "        \n",
    "                # Forward pass through Small Network\n",
    "                reembedded_output = small_network(roberta_embeddings)\n",
    "        \n",
    "                # Forward pass through Inverse Network\n",
    "                reconstructed_output = inverse_network(reembedded_output)\n",
    "        \n",
    "                # Compute Reconstruction Loss\n",
    "                reconstruction_loss = F.mse_loss(reconstructed_output, roberta_embeddings)\n",
    "                inverse_network_optimizer.zero_grad()\n",
    "                reconstruction_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(inverse_network.parameters(), max_norm=1.0)\n",
    "                inverse_network_optimizer.step()\n",
    "            print(f\"  Inverse Network Train Loss: {reconstruction_loss:.4f}\")\n",
    "        \n",
    "    # Step 1: Train Small Network to Remove Concept Information\n",
    "    small_network.train()\n",
    "    inverse_network.eval()\n",
    "    for _ in range(num_small_epochs):\n",
    "        total_small_network_loss = 0\n",
    "        for batch in tqdm(train_loader_small_network, desc=\"Training Small Network\"):\n",
    "            input_ids, attention_mask = [x.to(device) for x in batch]\n",
    "    \n",
    "            # Forward pass through RoBERTa\n",
    "            with torch.no_grad():\n",
    "                roberta_output = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                roberta_embeddings = roberta_output.last_hidden_state\n",
    "    \n",
    "            # Forward pass through Small Network\n",
    "            reembedded_output = small_network(roberta_embeddings)\n",
    "            if resconstruction:\n",
    "                # Forward pass through Inverse Network\n",
    "                reconstructed_output = inverse_network(reembedded_output)\n",
    "    \n",
    "            # Compute KL divergence loss\n",
    "            logits = concept_classifier(input_ids=None, attention_mask=None, embeddings=reembedded_output, return_probs=False)\n",
    "            kl_loss = kl_uniform_loss(logits, num_classes=2)\n",
    "    \n",
    "            # Combine losses for Small Network\n",
    "            if resconstruction:\n",
    "                total_loss = combined_loss(kl_loss, roberta_embeddings, reconstructed_output.detach(), lambda_reconstruction=lambda_reconstruction)  # Detach reconstructed_output\n",
    "            else:\n",
    "                total_loss= kl_loss\n",
    "                \n",
    "            # Backpropagation for Small Network\n",
    "            small_network_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(small_network.parameters(), max_norm=1.0)\n",
    "            small_network_optimizer.step()\n",
    "    \n",
    "            total_small_network_loss += total_loss.item()\n",
    "    \n",
    "        avg_small_network_loss = total_small_network_loss / len(train_loader_small_network)\n",
    "        print(f\"  Small Network Train Loss: {avg_small_network_loss:.4f}\")\n",
    "        \n",
    "        torch.save(small_network.state_dict(), f\"Shuo_small_network_ep_{epoch}.pt\")\n",
    "        torch.save(inverse_network.state_dict(), f\"Shuo_inverse_network_ep_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "274ccb26-1e64-48b4-afd2-7ea0746c7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收敛性曲线\n",
    "# small 400 steps IMDB\n",
    "# pmnet      0.0692  0.0442  0.0159  0.0163  0.0069  0.0068\n",
    "# distilbert 0.0665  0.0405  0.0136  0.0136  0.0061  0.0061\n",
    "# roberta    0.0583  0.0298  0.0105  0.0105  0.0051  0.0051\n",
    "\n",
    "# small 300 steps yelp\n",
    "# pmnet      0.0535  0.0511  0.0302  0.0302  0.0141  0.0141\n",
    "# distilbert 0.0444  0.0426  0.0244  0.0244  0.0142  0.0142\n",
    "# roberta    0.0346  0.0291  0.0173  0.0173  0.0110  0.0110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02af87c8-3347-45d1-b4fd-8c777bd18311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|█████████████████████████████████████████████| 80/80 [00:01<00:00, 45.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 58.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1894\n",
      "  Validation Accuracy: 0.9344\n",
      "  Validation loss improved from inf to 0.1894. Saving model...\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|█████████████████████████████████████████████| 80/80 [00:01<00:00, 45.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 56.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.2801\n",
      "  Validation Accuracy: 0.8844\n",
      "  No improvement in validation loss for 1/5 epochs.\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|█████████████████████████████████████████████| 80/80 [00:01<00:00, 46.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1973\n",
      "  Validation Accuracy: 0.9344\n",
      "  No improvement in validation loss for 2/5 epochs.\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|█████████████████████████████████████████████| 80/80 [00:01<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 51.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.2172\n",
      "  Validation Accuracy: 0.9187\n",
      "  No improvement in validation loss for 3/5 epochs.\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|█████████████████████████████████████████████| 80/80 [00:01<00:00, 47.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 68.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1896\n",
      "  Validation Accuracy: 0.9281\n",
      "  No improvement in validation loss for 4/5 epochs.\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|█████████████████████████████████████████████| 80/80 [00:01<00:00, 77.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 84.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.2348\n",
      "  Validation Accuracy: 0.9156\n",
      "  No improvement in validation loss for 5/5 epochs.\n",
      "Early stopping triggered.\n",
      "Best Sentiment Classifier model saved!\n",
      "0.05298670399934054\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Freeze Small Network and Train Sentiment Classifier\n",
    "MARGIN=0.0\n",
    "lamda=0.1\n",
    "DEBIASING_MODULE=True\n",
    "\n",
    "roberta_sentiment_classifier = RobertaSentimentClassifier(num_labels=2, model_name=model_name, small_network=small_network, debiasing_module=DEBIASING_MODULE).to(device)\n",
    "# Freeze the parameters of the RoBERTa backbone\n",
    "for param in roberta_sentiment_classifier.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if roberta_sentiment_classifier.small_network is not None:\n",
    "    for param in roberta_sentiment_classifier.small_network.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Ensure all additional layers are trainable\n",
    "for param in roberta_sentiment_classifier.swiglu.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in roberta_sentiment_classifier.projection.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in roberta_sentiment_classifier.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimizer for trainable parameters\n",
    "trainable_params = [\n",
    "    {'params': roberta_sentiment_classifier.swiglu.parameters()},\n",
    "    {'params': roberta_sentiment_classifier.projection.parameters()},\n",
    "    {'params': roberta_sentiment_classifier.fc.parameters()}\n",
    "]\n",
    "\n",
    "sentiment_optimizer = optim.AdamW(trainable_params, lr=0.0003)\n",
    "\n",
    "# Loss function remains the same\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if roberta_sentiment_classifier.small_network is not None:\n",
    "    for param in roberta_sentiment_classifier.small_network.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "num_classifier_epochs=50\n",
    "patience = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "wait = 0\n",
    "\n",
    "train_losses_sentiment = []\n",
    "val_losses = []\n",
    "small_network.eval()\n",
    "for epoch in range(num_classifier_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_classifier_epochs}\")\n",
    "\n",
    "    roberta_sentiment_classifier.train()\n",
    "    total_sentiment_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader_sentiment, desc=\"Training Sentiment Classifier\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        sentiment_optimizer.zero_grad()\n",
    "        logits, cos_sim = roberta_sentiment_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss_ce = F.cross_entropy(logits, labels)\n",
    "        loss_contrastive = contrastive_loss(cos_sim, margin=MARGIN, reverse=REVERSE)\n",
    "        loss=loss_ce+lamda*loss_contrastive\n",
    "        loss.backward()\n",
    "        sentiment_optimizer.step()\n",
    "\n",
    "        total_sentiment_loss += loss.item()\n",
    "\n",
    "    avg_sentiment_loss = total_sentiment_loss / len(train_loader_sentiment)\n",
    "    train_losses_sentiment.append(avg_sentiment_loss)\n",
    "    print(f\"  Sentiment Classifier Train Loss: {avg_sentiment_loss:.4f}\")\n",
    "\n",
    "    #############################################################################\n",
    "    \n",
    "    # Validation Phase\n",
    "    roberta_sentiment_classifier.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader_sentiment, desc=\"Validating Sentiment Classifier\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits, cos_sim = roberta_sentiment_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            total_val_loss += F.cross_entropy(logits, labels).item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader_sentiment)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f\"  Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = roberta_sentiment_classifier.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        print(f\"  No improvement in validation loss for {wait}/{patience} epochs.\")\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state:\n",
    "    roberta_sentiment_classifier.load_state_dict(best_model_state)\n",
    "    torch.save(roberta_sentiment_classifier.state_dict(), \"Shuo_sentiment_classifier.pt\")\n",
    "    print(\"Best Sentiment Classifier model saved!\")\n",
    "end_time=time.time()\n",
    "print((end_time-start_time)/1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "506cdfe4-d13d-421e-ba53-713b695d6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set: 100%|███████████████████████████████████████████████████████| 25/25 [00:00<00:00, 56.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9200\n",
      "Precision: 0.9421\n",
      "Recall: 0.8950\n",
      "F1 Score: 0.9179\n",
      "0.0011224699020385741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "b=time.time()\n",
    "# Evaluate the sentiment classifier\n",
    "roberta_sentiment_classifier.eval()\n",
    "\n",
    "# Initialize variables for storing predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for sample in tqdm(test_loader_sentiment, desc=\"Evaluating Test Set\"):\n",
    "        # Extract input_ids, attention_mask, and labels from the batch\n",
    "        input_ids = sample['input_ids'].to(device)\n",
    "        attention_mask = sample['attention_mask'].to(device)\n",
    "        labels = sample['label'].to(device)\n",
    "\n",
    "        # Get logits from the model\n",
    "        logits, cos_sim = roberta_sentiment_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Convert logits to predicted labels\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Append predictions and labels for metric computation\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute Metrics\n",
    "precision = precision_score(all_labels, all_preds, average=\"binary\")  # Use 'binary' for 2-class tasks\n",
    "recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "e=time.time()\n",
    "print((e-b)/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a9bc3-0bac-49ec-8ae2-8d56aced2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "92.00 91.96\n",
    "90.75 90.82\n",
    "91.50 91.71\n",
    "91.50 91.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25dd2a67-fe03-4604-94ca-fa69e0b2795a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4247027720.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[46], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    IMDB\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "ROBERTA\n",
    "    IMDB\n",
    "        b test\n",
    "            baseline\n",
    "                Accuracy: 0.7883\n",
    "                Precision: 0.9220\n",
    "                Recall: 0.6300\n",
    "                F1 Score: 0.7485\n",
    "            \n",
    "            ours\n",
    "                Accuracy: 0.8350\n",
    "                Precision: 0.7965\n",
    "                Recall: 0.9000\n",
    "                F1 Score: 0.8451\n",
    "                no reconstruction\n",
    "                    Accuracy: 0.8133\n",
    "                    Precision: 0.9017\n",
    "                    Recall: 0.7033\n",
    "                    F1 Score: 0.7903\n",
    "\n",
    "        a test\n",
    "            baseline\n",
    "                Accuracy: 0.8850\n",
    "                Precision: 0.8367\n",
    "                Recall: 0.9567\n",
    "                F1 Score: 0.8927\n",
    "            \n",
    "            ours\n",
    "                Accuracy: 0.8967\n",
    "                Precision: 0.8889\n",
    "                Recall: 0.9067\n",
    "                F1 Score: 0.8977\n",
    "\n",
    "    yelp\n",
    "        b test \n",
    "            baseline fl:Accuracy: 0.8900 F1 Score: 0.8952\n",
    "                Accuracy: 0.8925\n",
    "                Precision: 0.8651\n",
    "                Recall: 0.9300\n",
    "                F1 Score: 0.8964\n",
    "  \n",
    "            ours\n",
    "                Accuracy: 0.9150\n",
    "                Precision: 0.9323\n",
    "                Recall: 0.8950\n",
    "                F1 Score: 0.9133\n",
    "                no reconstruction\n",
    "                    Accuracy: 0.7975\n",
    "                    Precision: 0.7133\n",
    "                    Recall: 0.9950\n",
    "                    F1 Score: 0.8309\n",
    "\n",
    "        a test\n",
    "            baseline\n",
    "                Accuracy: 0.9375\n",
    "                Precision: 0.9730\n",
    "                Recall: 0.9000\n",
    "                F1 Score: 0.9351\n",
    "            \n",
    "            ours\n",
    "                Accuracy: 0.9475\n",
    "                Precision: 0.9686\n",
    "                Recall: 0.9250\n",
    "                F1 Score: 0.9463\n",
    "\n",
    "\n",
    "\n",
    "DistilBERT\n",
    "    IMDB\n",
    "        b test\n",
    "            baseline\n",
    "                Accuracy: 0.8167\n",
    "                Precision: 0.7987\n",
    "                Recall: 0.8467\n",
    "                F1 Score: 0.8220\n",
    "            ours\n",
    "                Accuracy: 0.8400\n",
    "                Precision: 0.8248\n",
    "                Recall: 0.8633\n",
    "                F1 Score: 0.8436\n",
    "                no reconstruction\n",
    "                    Accuracy: 0.8083\n",
    "                    Precision: 0.7697\n",
    "                    Recall: 0.8800\n",
    "                    F1 Score: 0.8212\n",
    "        \n",
    "        \n",
    "        a test\n",
    "            baseline\n",
    "                Accuracy: 0.8400\n",
    "                Precision: 0.7982\n",
    "                Recall: 0.9100\n",
    "                F1 Score: 0.8505\n",
    "            ours\n",
    "                Accuracy: 0.8550\n",
    "                Precision: 0.8562\n",
    "                Recall: 0.8533\n",
    "                F1 Score: 0.8548\n",
    "\n",
    "    yelp\n",
    "        b test\n",
    "            baseline\n",
    "                Accuracy: 0.8975\n",
    "                Precision: 0.8472\n",
    "                Recall: 0.9700\n",
    "                F1 Score: 0.9044\n",
    "            \n",
    "            ours\n",
    "                Accuracy: 0.9200\n",
    "                Precision: 0.9078\n",
    "                Recall: 0.9350\n",
    "                F1 Score: 0.9212\n",
    "                no reconstruction\n",
    "                    Accuracy: 0.9150\n",
    "                    Precision: 0.9611\n",
    "                    Recall: 0.8650\n",
    "                    F1 Score: 0.9105\n",
    "\n",
    "        a test\n",
    "            baseline\n",
    "                Accuracy: 0.9475\n",
    "                Precision: 0.9453\n",
    "                Recall: 0.9500\n",
    "                F1 Score: 0.9476\n",
    "\n",
    "            \n",
    "            ours\n",
    "                Accuracy: 0.9525\n",
    "                Precision: 0.9641\n",
    "                Recall: 0.9400\n",
    "                F1 Score: 0.9519\n",
    "\n",
    "MPnet\n",
    "    IMDB\n",
    "        b test\n",
    "            baseline\n",
    "                Accuracy: 0.7933\n",
    "                Precision: 0.7699\n",
    "                Recall: 0.8367\n",
    "                F1 Score: 0.8019\n",
    "            ours\n",
    "                Accuracy: 0.8150\n",
    "                Precision: 0.8247\n",
    "                Recall: 0.8000\n",
    "                F1 Score: 0.8122\n",
    "                no reconstruction\n",
    "                    Accuracy: 0.7983\n",
    "                    Precision: 0.8327\n",
    "                    Recall: 0.7467\n",
    "                    F1 Score: 0.7873\n",
    "\n",
    "        a test\n",
    "            baseline\n",
    "                Accuracy: 0.8733\n",
    "                Precision: 0.8972\n",
    "                Recall: 0.8433\n",
    "                F1 Score: 0.8694\n",
    "\n",
    "            ours\n",
    "                Accuracy: 0.8883\n",
    "                Precision: 0.8923\n",
    "                Recall: 0.8833\n",
    "                F1 Score: 0.8878\n",
    "\n",
    "    yelp\n",
    "        b test\n",
    "            baseline\n",
    "                Accuracy: 0.8900\n",
    "                Precision: 0.9432\n",
    "                Recall: 0.8300\n",
    "                F1 Score: 0.8830\n",
    "    \n",
    "            ours\n",
    "                Accuracy: 0.9075\n",
    "                Precision: 0.9137\n",
    "                Recall: 0.9000\n",
    "                F1 Score: 0.9068\n",
    "                no reconstruction\n",
    "                    Accuracy: 0.9025\n",
    "                    Precision: 0.9497\n",
    "                    Recall: 0.8500\n",
    "                    F1 Score: 0.8971\n",
    "\n",
    "        a test\n",
    "            baseline\n",
    "                Accuracy: 0.9275\n",
    "                Precision: 0.8869\n",
    "                Recall: 0.9800\n",
    "                F1 Score: 0.9311\n",
    "\n",
    "            ours\n",
    "                Accuracy: 0.9500\n",
    "                Precision: 0.9500\n",
    "                Recall: 0.9500\n",
    "                F1 Score: 0.9500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "021ad7b5-c178-457d-bee8-d8c08613bad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09609375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123/1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f7522-a33e-4d72-ae71-dba2e290fdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f453d-090e-4928-926f-972d0eff34f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "354e46da-3988-4db8-987e-d7367b103cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|███████████████████████████████████████████████████| 80/80 [00:00<00:00, 136.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|█████████████████████████████████████████████████| 20/20 [00:00<00:00, 187.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1780\n",
      "  Validation Accuracy: 0.9344\n",
      "  Validation loss improved from inf to 0.1780. Saving model...\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|███████████████████████████████████████████████████| 80/80 [00:00<00:00, 168.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.2399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|█████████████████████████████████████████████████| 20/20 [00:00<00:00, 174.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.2102\n",
      "  Validation Accuracy: 0.9313\n",
      "  No improvement in validation loss for 1/5 epochs.\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|███████████████████████████████████████████████████| 80/80 [00:00<00:00, 107.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.2031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 68.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1705\n",
      "  Validation Accuracy: 0.9375\n",
      "  Validation loss improved from 0.1780 to 0.1705. Saving model...\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:01<00:00, 66.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 67.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1983\n",
      "  Validation Accuracy: 0.9250\n",
      "  No improvement in validation loss for 1/5 epochs.\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:01<00:00, 65.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 67.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1791\n",
      "  Validation Accuracy: 0.9281\n",
      "  No improvement in validation loss for 2/5 epochs.\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:01<00:00, 65.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 67.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1689\n",
      "  Validation Accuracy: 0.9344\n",
      "  Validation loss improved from 0.1705 to 0.1689. Saving model...\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:01<00:00, 66.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 70.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1835\n",
      "  Validation Accuracy: 0.9281\n",
      "  No improvement in validation loss for 1/5 epochs.\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:00<00:00, 87.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 93.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1776\n",
      "  Validation Accuracy: 0.9250\n",
      "  No improvement in validation loss for 2/5 epochs.\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:00<00:00, 92.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 90.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.2096\n",
      "  Validation Accuracy: 0.9250\n",
      "  No improvement in validation loss for 3/5 epochs.\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|███████████████████████████████████████████████████| 80/80 [00:00<00:00, 177.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|█████████████████████████████████████████████████| 20/20 [00:00<00:00, 193.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1892\n",
      "  Validation Accuracy: 0.9250\n",
      "  No improvement in validation loss for 4/5 epochs.\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier: 100%|████████████████████████████████████████████████████| 80/80 [00:00<00:00, 84.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment Classifier Train Loss: 0.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Sentiment Classifier: 100%|██████████████████████████████████████████████████| 20/20 [00:00<00:00, 79.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.1768\n",
      "  Validation Accuracy: 0.9281\n",
      "  No improvement in validation loss for 5/5 epochs.\n",
      "Early stopping triggered.\n",
      "Best Sentiment Classifier model saved!\n",
      "0.011106026731431485\n"
     ]
    }
   ],
   "source": [
    "## baseline\n",
    "focal_loss=False\n",
    "\n",
    "baseline_sentiment_classifier = RobertaSentimentClassifier(num_labels=2, model_name=model_name, small_network=None).to(device)\n",
    "# Freeze the parameters of the RoBERTa backbone\n",
    "for param in baseline_sentiment_classifier.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Ensure all additional layers are trainable\n",
    "for param in baseline_sentiment_classifier.swiglu.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in baseline_sentiment_classifier.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in baseline_sentiment_classifier.projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimizer for trainable parameters\n",
    "trainable_params = [\n",
    "    {'params': baseline_sentiment_classifier.projection.parameters()},\n",
    "    {'params': baseline_sentiment_classifier.swiglu.parameters()},\n",
    "    {'params': baseline_sentiment_classifier.fc.parameters()}\n",
    "]\n",
    "\n",
    "baseline_optimizer = optim.AdamW(trainable_params, lr=0.0003)\n",
    "\n",
    "if focal_loss:\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2, reduction='mean')  # 你可以根据需求调整超参数\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_classifier_epochs=50\n",
    "# Early stopping and metrics\n",
    "patience = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "wait = 0\n",
    "\n",
    "train_losses_sentiment = []\n",
    "val_losses = []\n",
    "\n",
    "b=time.time()\n",
    "for epoch in range(num_classifier_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_classifier_epochs}\")\n",
    "\n",
    "    baseline_sentiment_classifier.train()\n",
    "    total_sentiment_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader_sentiment, desc=\"Training Sentiment Classifier\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        baseline_optimizer.zero_grad()\n",
    "        logits, cos_sim = baseline_sentiment_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        baseline_optimizer.step()\n",
    "\n",
    "        total_sentiment_loss += loss.item()\n",
    "\n",
    "    avg_sentiment_loss = total_sentiment_loss / len(train_loader_sentiment)\n",
    "    train_losses_sentiment.append(avg_sentiment_loss)\n",
    "    print(f\"  Sentiment Classifier Train Loss: {avg_sentiment_loss:.4f}\")\n",
    "\n",
    "    #############################################################################\n",
    "    \n",
    "    # Validation Phase\n",
    "    baseline_sentiment_classifier.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader_sentiment, desc=\"Validating Sentiment Classifier\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits, cos_sim = baseline_sentiment_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            total_val_loss += F.cross_entropy(logits, labels).item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader_sentiment)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f\"  Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = baseline_sentiment_classifier.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        print(f\"  No improvement in validation loss for {wait}/{patience} epochs.\")\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state:\n",
    "    baseline_sentiment_classifier.load_state_dict(best_model_state)\n",
    "    torch.save(baseline_sentiment_classifier.state_dict(), \"Shuo_sentiment_classifier_baseline.pt\")\n",
    "    print(\"Best Sentiment Classifier model saved!\")\n",
    "\n",
    "e=time.time()\n",
    "print((e-b)/1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71e8af74-d452-420c-b0b6-1af78b1a8bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set: 100%|██████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 61.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9050\n",
      "Precision: 0.8682\n",
      "Recall: 0.9550\n",
      "F1 Score: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "b=time.time()\n",
    "# Evaluate the baseline sentiment classifier\n",
    "baseline_sentiment_classifier.eval()\n",
    "\n",
    "# Initialize variables for storing predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for sample in tqdm(test_loader_sentiment, desc=\"Evaluating Test Set\"):\n",
    "        # Extract input_ids, attention_mask, and labels from the batch\n",
    "        input_ids = sample['input_ids'].to(device)\n",
    "        attention_mask = sample['attention_mask'].to(device)\n",
    "        labels = sample['label'].to(device)\n",
    "\n",
    "        # Get logits from the model\n",
    "        logits, cos_sim = baseline_sentiment_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Convert logits to predicted labels\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Append predictions and labels for metric computation\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute Metrics\n",
    "precision = precision_score(all_labels, all_preds, average=\"binary\")  # Use 'binary' for 2-class tasks\n",
    "recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "e=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ffde530-d7ed-4e96-bb45-c7d891eb53d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019940930604934693"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(e-b)/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93a427a1-99d5-4818-b0f8-9fdc9c842663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Softmax over logits to get probabilities\n",
    "        inputs = F.softmax(inputs, dim=-1)\n",
    "        \n",
    "        # One-hot encoding of targets\n",
    "        targets = F.one_hot(targets, num_classes=inputs.size(1))\n",
    "        \n",
    "        # Cross entropy loss component\n",
    "        cross_entropy_loss = -targets * torch.log(inputs)\n",
    "        \n",
    "        # Focal loss component\n",
    "        loss = self.alpha * (1 - inputs) ** self.gamma * cross_entropy_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.sum() / targets.size(0)\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f6ed0-961a-4c8b-a7ca-4bcd622eeb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
